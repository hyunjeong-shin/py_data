{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 출현빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gugu9999\\anaconda3\\envs\\test\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "것(1531) 말(1071) 그(1066) 사람(538) 생각(376) 일(303) 안(284) 상의(275) 그것(270) 이(266) 수(261) 때(252) 남(225) 선생(207) 나(197) 때문(195) 거(192) 내(185) 우리(177) 집(168) 감(165) 자신(164) 듯(163) 놈(161) 곳(160) 얼굴(158) 얘기(158) 명(153) 아이(149) 또(142) 어디(142) 양현(141) 뭐(139) 눈(138) 소리(137) 무슨(131) 몽치(129) 알(129) 산(129) 왜(127) 네(121) 너(119) 홍(118) 하나(117) 두(114) 환국(113) 영광(111) 영선(106) 모두(104) 방(102) \n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "# utf-16 인코딩으로 파일을 열고 글자를 출력하기 --- (※1)\n",
    "fp = codecs.open(\"BEXX0014.txt\", \"r\", encoding=\"utf-16\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "body = soup.select_one(\"text > body \")\n",
    "text = body.getText()\n",
    "# 텍스트를 한 줄씩 처리하기 --- (※2)\n",
    "twitter = Twitter()\n",
    "word_dic = {}\n",
    "lines = text.split(\"\\r\\n\")\n",
    "for line in lines:\n",
    "    malist = twitter.pos(line)\n",
    "    for word in malist:\n",
    "        if word[1] == \"Noun\": #  명사 확인하기 --- (※3)\n",
    "            if not (word[0] in word_dic):\n",
    "                word_dic[word[0]] = 0\n",
    "            word_dic[word[0]] += 1 # 카운트하기\n",
    "# 많이 사용된 명사 출력하기 --- (※4)\n",
    "keys = sorted(word_dic.items(), key=lambda x:x[1], reverse=True)\n",
    "for word, count in keys[:50]:\n",
    "    print(\"{0}({1}) \".format(word, count), end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7902ef6e6c57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTwitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# utf-16 인코딩으로 파일을 열고 글자를 출력하기 --- (※1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BEXX0014.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-16\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Twitter\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "# utf-16 인코딩으로 파일을 열고 글자를 출력하기 --- (※1)\n",
    "fp = codecs.open(\"BEXX0014.txt\", \"r\", encoding=\"utf-16\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "body = soup.select_one(\"text > body \")\n",
    "text = body.getText()\n",
    "\n",
    "# 텍스트를 한 줄씩 처리하기 --- (※2)\n",
    "twitter = Twitter()\n",
    "results = []\n",
    "lines = text.split(\"\\r\\n\")\n",
    "for line in lines:\n",
    "    # 형태소 분석하기 --- (※3)\n",
    "    # 단어의 기본형 사용\n",
    "    \n",
    "    malist = twitter.pos(line, norm=True, stem=True)\n",
    "    r = []\n",
    "    \n",
    "    for word in malist:\n",
    "        # 어미/조사/구두점 등은 대상에서 제외 \n",
    "        if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "    rl = (\" \".join(r)).strip()\n",
    "    results.append(rl)\n",
    "    print(rl)\n",
    "    \n",
    "    \n",
    "# 파일로 출력하기  --- (※4)\n",
    "wakati_file = 'toji.wakati'\n",
    "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(results))\n",
    "    \n",
    "    \n",
    "# Word2Vec 모델 만들기 --- (※5)\n",
    "data = word2vec.LineSentence(wakati_file)\n",
    "model = word2vec.Word2Vec(data, \n",
    "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
    "model.save(\"toji.model\")\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gugu9999\\anaconda3\\envs\\test\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('말년', 0.9010502696037292),\n",
       " ('한', 0.8854928612709045),\n",
       " ('집어치우다', 0.8807700276374817),\n",
       " ('천도', 0.8802703619003296),\n",
       " ('파', 0.8782978057861328),\n",
       " ('우떻', 0.8774944543838501),\n",
       " ('지지리', 0.877079963684082),\n",
       " ('함부로', 0.8713259100914001),\n",
       " ('억울하다', 0.8709316849708557),\n",
       " ('둘', 0.8680309057235718)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec.load(\"toji.model\")\n",
    "\n",
    "model.most_similar(positive=[\"땅\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gugu9999\\anaconda3\\envs\\test\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('보라', 0.833906888961792),\n",
       " ('드나들다', 0.8335009813308716),\n",
       " ('어멈', 0.8257094621658325),\n",
       " ('혜화동', 0.8184428215026855),\n",
       " ('하동', 0.8162745833396912),\n",
       " ('여관', 0.8144456148147583),\n",
       " ('들르다', 0.8117949962615967),\n",
       " ('거기', 0.8071713447570801),\n",
       " ('갈다', 0.8047260046005249),\n",
       " ('묵고', 0.7982692718505859)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"집\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram: 0.7619047619047619 ['오늘', '강남', '남에', '에서', '서 ', ' 맛', '맛있', '는 ', ' 스', '스파', '파게', '게티', ' 먹', '먹었', '었다', '다.']\n",
      "3-gram: 0.45 ['강남에', '남에서', '에서 ', ' 맛있', ' 스파', '스파게', '파게티', ' 먹었', '었다.']\n"
     ]
    }
   ],
   "source": [
    "def ngram(s, num):\n",
    "    res = []\n",
    "    slen = len(s) - num + 1\n",
    "    for i in range(slen):\n",
    "        ss = s[i:i+num]\n",
    "        res.append(ss)\n",
    "    return res\n",
    "def diff_ngram(sa, sb, num):\n",
    "    a = ngram(sa, num)\n",
    "    b = ngram(sb, num)\n",
    "    r = []\n",
    "    cnt = 0\n",
    "    for i in a:\n",
    "        for j in b:\n",
    "            if i == j:\n",
    "                cnt += 1\n",
    "                r.append(i)\n",
    "    return cnt / len(a), r\n",
    "a = \"오늘 강남에서 맛있는 스파게티를 먹었다.\"\n",
    "b = \"강남에서 먹었던 오늘의 스파게티는 맛있었다.\"\n",
    "# 2-gram\n",
    "r2, word2 = diff_ngram(a, b, 2)\n",
    "print(\"2-gram:\", r2, word2)\n",
    "# 3-gram\n",
    "r3, word3  = diff_ngram(a, b, 3)\n",
    "print(\"3-gram:\", r3, word3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP로 텍스트분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어를 ID로 변환하고 출현횟수 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "root_dir = \"./newstext\"\n",
    "dic_file = root_dir + \"/word-dic.json\"\n",
    "data_file = root_dir + \"/data.json\"\n",
    "data_file_min = root_dir + \"/data-mini.json\"\n",
    "# 어구를 자르고 ID로 변환하기 ---(※1)\n",
    "word_dic = { \"_MAX\": 0 }\n",
    "def text_to_ids(text):\n",
    "    text = text.strip()\n",
    "    words = text.split(\" \")\n",
    "    result = []\n",
    "    for n in words:\n",
    "        n = n.strip()\n",
    "        if n == \"\": continue\n",
    "        if not n in word_dic:\n",
    "            wid = word_dic[n] = word_dic[\"_MAX\"]\n",
    "            word_dic[\"_MAX\"] += 1\n",
    "            print(wid, n)\n",
    "        else:\n",
    "            wid = word_dic[n]\n",
    "        result.append(wid)\n",
    "    return result\n",
    "# 파일을 읽고 고정 길이의 배열 리턴하기 ---(※2)\n",
    "def file_to_ids(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        text = f.read()\n",
    "        return text_to_ids(text)\n",
    "# 딕셔너리에 단어 모두 등록하기 --- (※3)\n",
    "def register_dic():\n",
    "    files = glob.glob(root_dir+\"/*/*.wakati\", recursive=True)\n",
    "    for i in files:\n",
    "        file_to_ids(i)\n",
    "# 파일 내부의 단어 세기 --- (※4)\n",
    "def count_file_freq(fname):\n",
    "    cnt = [0 for n in range(word_dic[\"_MAX\"])]\n",
    "    with open(fname,\"r\") as f:\n",
    "        text = f.read().strip()\n",
    "        ids = text_to_ids(text)\n",
    "        for wid in ids:\n",
    "            cnt[wid] += 1\n",
    "    return cnt\n",
    "# 카테고리마다 파일 읽어 들이기 --- (※5)\n",
    "def count_freq(limit = 0):\n",
    "    X = []\n",
    "    Y = []\n",
    "    max_words = word_dic[\"_MAX\"]\n",
    "    cat_names = []\n",
    "    for cat in os.listdir(root_dir):\n",
    "        cat_dir = root_dir + \"/\" + cat\n",
    "        if not os.path.isdir(cat_dir): continue\n",
    "        cat_idx = len(cat_names)\n",
    "        cat_names.append(cat)\n",
    "        files = glob.glob(cat_dir+\"/*.wakati\")\n",
    "        i = 0\n",
    "        for path in files:\n",
    "            print(path)\n",
    "            cnt = count_file_freq(path)\n",
    "            X.append(cnt)\n",
    "            Y.append(cat_idx)\n",
    "            if limit > 0:\n",
    "                if i > limit: break\n",
    "                i += 1\n",
    "    return X,Y\n",
    "# 단어 딕셔너리 만들기 --- (※5)\n",
    "if os.path.exists(dic_file):\n",
    "    word_dic = json.load(open(dic_file))\n",
    "else:\n",
    "    register_dic()\n",
    "    json.dump(word_dic, open(dic_file,\"w\"))\n",
    "# 벡터를 파일로 출력하기 --- (※6)\n",
    "# 테스트 목적의 소규모 데이터 만들기\n",
    "X, Y = count_freq(20)\n",
    "json.dump({\"X\": X, \"Y\": Y}, open(data_file_min,\"w\"))\n",
    "# 전체 데이터를 기반으로 데이터 만들기\n",
    "X, Y = count_freq()\n",
    "json.dump({\"X\": X, \"Y\": Y}, open(data_file,\"w\"))\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP로 텍스트 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, metrics\n",
    "import json\n",
    "max_words = 67395 # 입력 단어 수: word-dic.json 파일 참고\n",
    "nb_classes = 9    # 9개의 카테고리\n",
    "batch_size = 64 \n",
    "nb_epoch = 20\n",
    "# MLP 모델 생성하기 --- (※1)\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(max_words,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "# 데이터 읽어 들이기--- (※2)\n",
    "data = json.load(open(\"./newstext/data-mini.json\")) \n",
    "#data = json.load(open(\"./newstext/data.json\"))\n",
    "X = data[\"X\"] # 텍스트를 나타내는 데이터\n",
    "Y = data[\"Y\"] # 카테고리 데이터\n",
    "# 학습하기 --- (※3)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "print(len(X_train),len(Y_train))\n",
    "model = KerasClassifier(\n",
    "    build_fn=build_model, \n",
    "    nb_epoch=nb_epoch, \n",
    "    batch_size=batch_size)\n",
    "model.fit(X_train, Y_train)\n",
    "# 예측하기 --- (※4)\n",
    "y = model.predict(X_test)\n",
    "ac_score = metrics.accuracy_score(Y_test, y)\n",
    "cl_report = metrics.classification_report(Y_test, y)\n",
    "print(\"정답률 =\", ac_score)\n",
    "print(\"리포트 =\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
